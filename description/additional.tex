\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=3cm]{geometry}

\begin{document}

\begin{titlepage}
\vspace*{3cm}
\begin{center}


{\LARGE
beachmat: a C++ API for data access from R matrix types
\par}

\vspace{0.75cm}

{\Large
    \textsc{Supplementary Materials}
\par
}
\vspace{0.75cm}

\large
by


\vspace{0.75cm}
Aaron T. L. Lun$^1$

\vspace{1cm}
\begin{minipage}{0.9\textwidth}
\begin{flushleft}
$^1$Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom \\[6pt]
\end{flushleft}
\end{minipage}

\vspace{1.5cm}
{\large \today{}}

\vspace*{\fill}
\end{center}
\end{titlepage}

\providecommand{\myceil}[1]{\left \lceil #1 \right \rceil }

\section{Optimizing HDF5 chunk cache parameters}

\subsection{Overview}
Matrix data in HDF5 files can be partitioned into chunks \cite{hdf5chunk}, where each chunk is a submatrix of the same dimensions.
When a particular row or column of the matrix is requested, the HDF5 library will read all chunks that are overlapped by the requested row/column.
Each chunk is read from disk in its entirety, even if only a subset of the chunk is overlapped by the requested row/column.
This is wasteful as more data needs to be processed than is required for the current request.
To improve performance, chunks are cached in memory once they are read.
This means that they do not have to be re-read if a neighbouring row/column (overlapping the same set of chunks) is requested.
We can tune the parameters of the HDF5 chunk cache to improve access to consecutive row/columns.

\subsection{Selecting the size of the chunk cache}
Assume that we have a chunk cache of size $S$ (in terms of data values; this will have to be multiplied by the size in bytes of each value to obtain the actual cache size).
Denote the number of rows and columns of our matrix as $R$ and $C$, respectively. 
We further assume that this data are stored in a chunked layout, where the number of rows and columns in each chunk is $P$ and $Q$, respectively.

The chunk size $S$ should be chosen so that it can hold all chunks lying across a row or column.
Each chunk contains $PQ$ values, and each row and column overlaps $\myceil{C/Q}$ and $\myceil{R/P}$ chunks, respectively.
This reflects the fact that each chunk is stored in its entirety, even at the right/bottom edges of the matrix where each chunk may not be completely filled.
For row access, this means that 
\begin{equation}
S \ge \myceil{C/Q}PQ \;, 
\end{equation}
i.e., all chunks overlapping a particular row are held in memory. Simliarly, for column access, 
\begin{equation}
S \ge \myceil{R/P}PQ \;.
\end{equation}
This ensures that all chunks overlapping a particular row or column are retained in memory.
Otherwise, if the cache size is too small, existing chunks in the cache will be evicted before the current row or column is fully read.
(By default, the HDF5 chunk cache operates on a first-in-first-out basis when full.)
This would preclude the use of the cache to rapidly load data for the next row or column.

\subsection{Choosing the number of slots}
The chunk cache works by hashing the chunks according to their location in a one-dimensional array.
Consider a ``meta-matrix'' of chunks, where each chunk in the original matrix constitutes one entry in the meta-matrix.
A meta-row consists of all chunks along one row of this meta-matrix.
Similarly, a meta-column consists of all chunks along one column of the meta-matrix. 
Denote the number of meta-rows and meta-columns as $R_0$ and $C_0$, respectively.
Now, flatten the meta-matrix into a one-dimensional array of chunks, organized in row-major format.
The first chunk is assigned a hash index of 1 (technically 0, but we will use 1-based indexing for simplicity here).
The next chunk is assigned a hash index of 2, and so on.
When the hash index exceeds the number of cache slots $N$, it is reset to 1 for the next chunk.
This continues until a hash index is assigned to all entries of the meta-matrix.

The hash indices are important as two chunks with the same hash index cannot co-exist in the chunk cache.
This means that $N$ must be carefully chosen to ensure that all chunks in a meta-row or a meta-column have unique hash indices.
Otherwise, an existing chunk in the cache will be evicted by a newer chunk with the same hash index.
This would require the re-reading of the data in the older chunk when accessing the next row/column, defeating the purpose of the cache.
For a meta-row, guaranteeing unique indices is trivial as $N$ only needs to be chosen to be greater than or equal to $C_0$.

For a meta-column, more care is required.
Consider the first chunk lying in the first meta-column with a hash index of 1.
The hash index of 1 will only return to the first meta-column when the sequence $\{1, \ldots, N\}$ has been repeated enough times $m$ such that $mN$ is the smallest multiple of $C_0$.
Prior to this, all assigned values of the first meta-column will be unique.
This is obvious because non-unique values of the hash index in the first meta-column would imply an existing repeating structure, which is not possible without repeating the hash index of 1.
(The same logic can be applied to any other value of the hash index in any other meta-column.)
As the number of rows covered by $mN$ is equal to $mN/C_0$, we simply need to choose $N$ such that $mN/C_0 \ge R_0$.
This ensures that the hash index can never repeat itself for a given meta-column before hash indices are assigned to all entries in the meta-matrix.
The most direct approach is to set $N = kC_0 + 1$  where $k \in \mathbf{N}^+$ and $kC_0 +1 \ge R_0$.
This guarantees that the lowest common multiple of $N$ and $C_0$ is their product, and that $mN/C_0 \ge R_0$.

\subsection{Choosing the chunk dimensions}
The preceding text assumes that the chunk dimensions $P$ and $Q$ were set beforehand.
This is the case for existing HDF5 files, where the chunk layout must be specified during data set creation.
However, for new files, we have an opportunity to choose the chunk dimensions to optimize data access.

The number of disk reads involved in accessing each row is equal to the number of chunks across a row of the matrix, i.e., $\myceil{C/Q}$.
Once these are loaded into the cache, no more reads are required to access the next $P-1$ rows, i.e., data are held in memory for all $P$ rows simultaneously.
For comparison, consider a layout containing row chunks, i.e., each chunk is an entire row.
This provides the most efficient access to any single row as one read obtains all and only that row's data.
To access $P$ rows from a row-chunked layout, $P$ reads would be required.
Thus, for accessing consecutive rows with $P \times Q$ chunks, it is possible to perform as well as or better than the row-chunked layout if 
\begin{equation}
\myceil{C/Q} \le P \;.
\end{equation}
The same logic applies to column access, where $\myceil{R/P}$ reads are required to store data for $Q$ consecutive columns.
A layout with pure column chunks would require $Q$ reads to access the same data.
If 
\begin{equation}
\myceil{R/P} \le Q \;,
\end{equation}
we can also outperform the column-chunked layout for accessing data in consecutive columns.
Thus, by choosing $P$ and $Q$ to satisfy Inequalities~3 and 4, we can use a single file layout to achieve data access performance similar to or better than that of pure column- or row-based chunking.

Obviously, any arbitrarily large $P$ and $Q$ will satisfy Inequalities~3 and 4.
To avoid this, we use Inequalities~1 and 2 to determine the minimum chunk cache size.
The aim is to satisfy all of the inequalities while minimizing $S$ to reduce the memory overhead.
In general, this requires some numerical analysis to account for the discrete nature of the dimensions.
We obtain some approximate values by treating all variables as continuous, which allows us to satisfy all inequalities when $S \ge \max\{R\sqrt{C}, C\sqrt{R}\}$.
Once the smallest $S$ is obtained, the largest $P$ and $Q$ are easily determined.

\subsection{Worked example of choosing chunk settings}
Say that we have a matrix $X$ with 20000 rows and 50000 columns.
Our approach suggests that we should choose $S$ as being greater than $50000\sqrt{20000} \approx 7071068$ to optimize row and column access.
We set $S=8000000$ to simplify the later divisions, which corresponds to a in-memory space of 64 MB.
Our chunk dimensions are then $P = S/50000 = 160$ and $Q = S/20000 = 400$.
In practice, one may wish to cap the cache size at the cost of some performance, lest too much memory be requested.


\small
\bibliography{ref}
\bibliographystyle{unsrt}

\end{document}
