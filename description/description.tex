\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in,marginparwidth=2in]{geometry}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}

% clean citations
\usepackage{cite}

% hyperref makes references clicky. use \url{www.example.com} or \href{www.example.com}{description} to add a clicky url
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% improves typesetting in LaTeX
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% text layout - change as needed
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Remove % for double line spacing
%\usepackage{setspace} 
%\doublespacing

% use adjustwidth environment to exceed text width (see examples in text)
\usepackage{changepage}

% adjust caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% remove brackets from references
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% headrule, footrule and page numbers
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}

% use \textcolor{color}{text} for colored text (e.g. highlight to-do areas)
\usepackage{color}

% define custom colors (this one is for figure captions)
\definecolor{Gray}{gray}{.25}

% this is required to include graphics
\usepackage{graphicx}

% use if you want to put caption to the side of the figure - see example in text
\usepackage{sidecap}

% use for have text wrap around figures
\usepackage{wrapfig}
\usepackage[pscoord]{eso-pic}
\usepackage[fulladjust]{marginnote}
\reversemarginpar

% Adding multirow.
\usepackage{multirow}

% Other required things:
\usepackage{color}
\usepackage{subcaption}
\captionsetup[subfigure]{justification=centering}
\newcommand{\beachmat}{\textit{beachmat}}
\newcommand{\code}[1]{\texttt{#1}}

% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
    \textbf\newline{beachmat: a C++ API for data access from R matrix types}
}
\newline

% authors go here:
%\\
Aaron T. L. Lun\textsuperscript{1,*}
and friends
\\
\bigskip
\bf{1} Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom
\\
\bigskip
* aaron.lun@cruk.cam.ac.uk

\end{flushleft}

\section*{Abstract}
Blha blah blah

% now start line numbers
\linenumbers

\section*{Introduction}
Recent advances in single-cell RNA sequencing (scRNA-seq) technologies have led to an explosion in the quantity of data that can be generated in routine experiments.
Droplet-based methods such as Drop-Seq \cite{macosko2015highly}, inDrop \cite{klein2015droplet} and GemCode \cite{zheng2017massively} allow expression profiles to be captured for each of thousands to millions of cells.
It hardly needs to be said that this is a substantial amount of data -- the expression profile for each cell consists of a measure of expression for each transcriptionally active genomic feature, of which there are usually 10,000 to 40,000 in most common model organisms.
Careful computational analysis is critical to extract meaningful biology from these data, but the sheer volume strains existing pipelines and methods designed for single-cell data processing.
The challenge is compounded by the presence of large-scale projects such as the Human Cell Atlas \cite{regev2017human}, which aims to use single-cell `omics to profile every cell type in the human body.
Similar issues are encountered outside of transcriptomics, with single-cell ATAC-seq \cite{buenrostro2015single} and bisulfite sequencing \cite{smallwood2014single} providing region- to base-level resolution of biochemical events (chromatin accessibility and DNA methylation, respectively).
This results in even more data compared to gene-level expression values.

It is no exaggeration to say that the R programming language \cite{R} is the premier tool of choice for statistical data analysis.
R provides well-designed, rigorously-tested implementations of a large variety of statistical methods.
Its interactive nature makes it easy for newcomers to learn and lends itself to data exploration and research, while its programming features allow more experienced users to readily assemble complex analyses.
It is also extensible through the installation of optional packages, often contributed by the research community, which provide implementations of bespoke methods targeted to solve specific scientific problems.
In particular, the Bioconductor project \cite{gentleman2004bioconductor} supports a number of packages for biological data analyis, many of which focus on the processing of genomics data \cite{huber2015orchestrating}.
Packages are usually written in R but can also include compiled code (e.g., in C/C++ or Fortran), which is beneficial for computationally intensive tasks where high performance is required.
For C++ code, this process is facilitated by the \textit{Rcpp} package \cite{eddelbuettel2011seamless}, which simplifies the integration of package code with the R application programming interface (API).

In its simplest form, a scRNA-seq data set consists of a count matrix where each column is a cell, each row is a gene, and the value of each matrix entry is set to the quantified expression (e.g., number of mapped reads, transcripts-per-million) for that gene in that cell.
This can be most directly represented in R as a simple matrix, where each entry is explicitly stored in memory.
Alternatively, it can be stored as a sparse matrix using classes from the \textit{Matrix} package \cite{bates2017matrix}, which saves memory by only storing non-zero entries.
This exploits the fact that scRNA-seq protocols have low capture efficiencies \cite{grun2015design} -- RNA molecules are present in cells but are not reverse-transcribed to cDNA for sequencing, resulting in a preponderance of zeroes in the final count matrix.
Another option is to use file-backed representations such as those in the \textit{bigmemory} \cite{kane2013scalable} or \textit{HDF5Array} packages, where the data set is stored on disk and parts of it are extracted into memory upon request.
In each case, methods are provided in R for common operations such as subsetting, transposition and arithmetic, such that code written by users (or other developers) can be agnostic to the exact representation of the matrix.
This simplifies the development process and improves interoperability.

Unfortunately, for compiled code written in statically typed languages like C++, the details of the matrix representation must be known during compilation.
This makes it difficult to write a single, general piece of code that can be applied to many different representations.
Writing multiple versions for each representation is difficult and unsustainable when more representations become available.
The alternative is to perform all processing in R to exploit the availability of common methods.
However, this is an unappealing option for high-performance code.
For scRNA-seq data stored in matrices, consider the most common access pattern, i.e., looping across all cells or genes and performing operations on the cell- or gene-specific expression profiles.
If this was performed in R, the code within the loop would need to be re-interpreted at each of thousands or millions of iterations.
This increases the computational time required to perform analyses, which is inconvenient for small scripts, undesirable for interactive analyses and unacceptable for large simulation studies.
It would clearly be preferable to implement critical functions, loops and all, in compiled code wherever possible.

% One might think to simply move the inside of the loop in C++ to migitate the interpretation cost, while keeping the loop at the R level to access row- or column-level data.
% However, this involves some costs on its own, e.g., memory allocations that were previously one-offs need to be re-performed.
% If you need to share data across iterations, it will also involve more function calls to store relevant variables, so the cost of function calls is not avoided.

\section*{Description of the \beachmat{} API}

\subsection*{Overview of the API}
The \beachmat{} API uses C++ classes to provide a common interface for data access from R matrix representations.
We define a base class that implements common methods for all matrix representations.
Each specific representation is associated with a derived C++ class that provides customized implementations of the access methods.
The intention is for a user to pass in an R matrix of any type, in the form of an \code{RObject} instance from the \textit{Rcpp} API (Figure X).
A function is then called to produce its C++ equivalent, returning a pointer to the base class.
This pointer is the same regardless of the R representation and can be used in downstream code to achieve run-time polymorphism.

While the API is agnostic to the matrix representation, it still needs to know the type of data that is stored within the matrix.
We use C++ templating to recycle the code to define specific classes for common data types, i.e., logical, integer, double-precision floating point or character strings.
The same methods are available for all classes of each data type, which eases the cognitive burden on the developer.
Briefly, when access to a specific row or column (or a slice thereof) is requested, the API will fill a \textit{Rcpp}-style \code{Vector} object with corresponding data values from the matrix.
A request for a specific entry of the matrix will directly return the corresponding data value.

In the following text, we discuss some of the specifics of the \beachmat{} API implementation.
This includes the details of each matrix representation, its memory footprint and the computational time required for data access.

\subsection*{Performance with simple matrices}
By default, R stores matrices as one-dimensional arrays of length $N_rN_c$, where $N_r$ and $N_c$ are the number of rows and columns, respectively.
This is done in column-major format, i.e., the matrix entry $(x, y)$ corresponds to array element $x + N_ry$ (assuming zero-based indexing).
We refer to this format as a ``simple matrix''.
The simple matrix is easy to manipulate and the time required for data access is linear with respect to the number of rows/columns (Figure~\ref{fig:basetime}).
However, its memory footprint is directly proportional to its length.
For example, a double-precision matrix containing data for 10000 genes in each of one million cells would require 80 GB of RAM to store in memory.
This is currently not possible for most workstations, instead requiring dedicated high-performance computing resources.
Even smaller matrices will cause problems on systems with limited memory due to R's copy-on-write semantics.
Thus, the utility of simple matrices is limited to relatively small scRNA-seq data sets.

% While R doesn't copy if you don't modify, it _does_ copy if you do modify, rather tha overwriting the old version.
% Consider computing log2-normalized counts; it's four copies (first transpose, division, second transpose, log-transformation).

\begin{figure}[bt]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/base_col.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/base_row.pdf}
        \caption{}
    \end{subfigure}
    \caption{Time required for column or row access of simple matrices using the \beachmat{} or \textit{Rcpp} APIs.
        (a) Column access time with respect to increasing number of columns, for a matrix with 10000 rows.
        (b) Row access time with respect to increasing number of rows, for a matrix with 1000 columns.
        Access was timed after computing the column/row sums to ensure iteration across data values with each API.
        Each time represents the average of 10 simulations.
        Standard errors were negligible and are not shown.
        Intervals between the horizontal dotted lines represent 2-fold increases in time.
    }
    \label{fig:basetime}
\end{figure}

We compare the access speed of the \beachmat{} API to that of a reference implementation using only \textit{Rcpp}.
Both row and column access via \beachmat{} require 20-50\% more time compared to the reference (Figure~\ref{fig:basetime}).
This is expected as \beachmat{} is built on top of \textit{Rcpp}, so the former cannot be faster than the latter.
Another reason is that, at each row/column access, \beachmat{} copies the matrix data into a \code{Vector}.
In contrast, the reference implementation avoids the overhead of creating a new copy by simply iterating across the original data.
Our use of copying is deliberate as it simplifies the API and keeps it consistent across matrix representations. 
Iterating across the original data is complicated for sparse matrices (due to lack of memory locality) and impractical for file-based representations.
Moreover, copying is required anyway for complex operations that involve transformations and/or re-ordering of data, as well as for libraries such as LAPACK that accept a pointer to a contiguous block of memory.
This suggests that, in practice, \beachmat{}'s computational overhead can be ignored.

\subsection*{Performance with sparse matrices}
The \code{dgCMatrix} class from the \textit{Matrix} package stores sparse matrix data in compressed sparse column-orientated (CSC) format.
Consider that every non-zero entry in this matrix is characterized by a triplet: row $x$, column $y$ and value $v$.
To convert this into the CSC format, entries are sorted in order of increasing $x + N_ry$.
All entries with the same value of $y$ are now grouped together in the ordered sequence.
We refer to each column-based group as $G_y$, the entries of which are sorted internally in order of increasing $x$.
The representation is further compressed by discarding $y$ from each triplet.
All entries from the same column are at consecutive locations of the ordered sequence, so only the start position of $G_y$ on the sequence needs to be stored for each column.
(The end position of one column is simply the start position of the next column.)
This reduces memory usage to $s_IN_c + (s_I + s_v) N_{\ne 0}$ where $s_I$ is the size of an integer, $s_v$ is the size of a single data element and $N_{\ne 0}$ is the number of non-zero elements in the matrix.
For double-precision matrices with many rows, sparse matrices will be more memory-efficient than their simpler ``dense'' counterparts if the density of non-zero elements is less than $\approx66$\% (assuming 4-byte integers and 8-byte doubles). 

% Drops down to 33% for integer matrices, as Matrix needs to convert them to double-precision.

The CSC format simplifies data access by imposing structure on the non-zero entries.
When accessing a particular column $c$, all corresponding entries in $G_c$ can be quickly extracted by taking the relevant part of the ordered sequence.
For low-density sparse matrices, column access via \beachmat{} is even faster than access from simple matrices (Figure~\ref{fig:sparsecol}).
This is because only a few non-zero entries need to be copied -- the rest of the \code{Vector} can be rapidly filled with zeroes.
As the density of non-zero entries increases, column access becomes slower but is still comparable to that of simple matrices.
We note that the \textit{RcppArmadillo} package \cite{eddelbuettel2014arma} also handles sparse matrices via the \code{SpMat} class.
This provides faster column-level access than the \beachmat{} API as no copying of data is performed -- see above for a related discussion with simple matrices.

% A more fundamental point is that, if your data is highly sparse, you can design more efficient algorithms by just not iterating over the zeroes.
% In contrast, RcppArmadillo still involves iteration over the zero values; it just doesn't do any copying of them.
% If you only iterate over the non-zero entries, you would get highly efficient algorithms for simple operations.
% However, this is not palatable as it means you have to write one version of the code for sparse matrices and another version for dense matrices.

\begin{figure}[bt]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/sparse_col_density.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/sparse_col_ncol.pdf}
        \caption{}
    \end{subfigure}
    \caption{Column access times for CSC matrices using \beachmat{} (sparse) or \textit{RcppArmadillo} (arma), compared to access times for an equivalent simple matrix with \beachmat{} (dense).
        (a) Access times with respect to increasing density of non-zero entries (as a percentage of all entries), for a matrix with 10000 rows and 1000 columns.
        (b) Access times with respect to increasing number of columns, for a matrix with 10000 rows and 1\% non-zero entries.
        Horizontal dotted lines represent 2-fold increases in time.
    }
    \label{fig:sparsecol}
\end{figure}

Row-level access is more difficult in the CSC format as entries in the same row do not follow a predictable pattern.
If a row is requested, a binary search on $x$ needs to be performed within $G_y$ for each column $y$, which requires an average time proportional to $\log(N_r)$.
In contrast, obtaining the next element in a row of a dense matrix can be done in constant time by jumping $N_r$ elements ahead on the one-dimensional array.
To speed up row access for sparse matrices, we realized that the most common access pattern involves requests for consecutive rows.
If row $r$ is accessed, \beachmat{} will loop over all columns and cache the index the first value of $x$ in $G_y$ that is not less than $r$.
When $r+1$ is accessed, we simply need to check if each of the indices should be incremented by one.
This avoids the need to perform a new binary search and reduces the row access time substantially (Figure~\ref{fig:sparserow}a, b).
Even when the row access pattern is random, we mitigate the time penalty by checking if the requested row is greater than or less than the previous row for which indices are stored.
If greater, we use the stored indices to set the start of the binary search; if less, we use the indices to set the end of the search. 
This reduces the search space and the amount of computational work for large $N_r$.

% We don't use CSR format because dgRMatrix objects are not supported much by Matrix.
% Otherwise, we would probably recommend people to convert to dgRMatrix types if they know that they need row access later.
% Of course, this is a moot point if you need row _and_ column access in the same C++ function.

\begin{figure}[btp]
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/sparse_row_naive_density.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/sparse_row_naive_nrow.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/sparse_row_density.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/sparse_row_nrow.pdf}
        \caption{}
    \end{subfigure}
    \caption{Row access times for CSC matrices using a naive binary search or the improved caching method in \beachmat{} (a, b); and in comparison to an equivalent simple matrix in \beachmat{} (dense) (c, d).
        (a, c) Access times with respect to increasing density of non-zero entries, for a matrix with 10000 rows and 1000 columns.
        (b, d) Access times with respect to increasing number of rows, for a matrix with 1000 columns and 1\% non-zero entries.
        Horizontal dotted lines represent 2-fold increases in time.
    }
    \label{fig:sparserow}
\end{figure}


Despite these optimisations, row access with sparse matrices in \beachmat{} remains slower than that with simple matrices (Figure~\ref{fig:sparserow}c).
This is not surprising as there is simply less work to do with dense matrices.
The exception is with large matrices of low density (Figure~\ref{fig:sparserow}d), where the cost of CPU cache misses due to large jumps exceeds the cost of handling both $x$ and $v$ per non-zero entry.
Row access of either representation with \beachmat{} is also faster than that of sparse matrices with \textit{RcppArmadillo}.
For example, for a 10000-by-1000 sparse matrix with 1\% non-zero entries, \beachmat{} with sparse matrices takes 39.8 milliseconds to access each row; \beachmat{} with dense matrices takes 29.2 milliseconds; and \textit{RcppArmadillo} takes 1921.4 milliseconds.
These results motivate the use of \beachmat{} for data access from sparse matrices.

\subsection*{Performance with HDF5-based matrices}
For large, non-sparse matrices that do not fit into memory, the most obvious option is to store them on disk and load submatrices into memory as required.
We consider the use of the hierarchical data format (HDF5) \cite{hdf5}, which provides flexible and efficient storage of and access to large amounts of data in a filesystem-like format.
Each large matrix is stored as a dataset in a HDF5 file on disk, while in memory it is represented by a \code{HDF5Matrix} object from the \textit{HDF5Array} package.
The in-memory representation is very small -- fewer than 3 kilobytes in size -- and simply extracts data from disk upon request.
Each \code{HDF5Matrix} instance provides methods to mimic a real matrix object and allows users to manipulate the matrix in real time without the need to load all of the data into memory.
Compression of data in the HDF5 file also ensures that the on-disk footprint remains manageable throughout the course of the analysis.

The \beachmat{} API supports row- and column-level access from a \code{HDF5Matrix} instance.
This means that even very large data sets can be accessed in C++, using the same code for simple and sparse matrix representations.
However, access is necessarily slower than that from a simple matrix as the data need to be read from disk at regular intervals.
We observed a 20-fold increase in the time required to access each column and a 40-fold increase in the time required to access each row (Figure~\ref{fig:hdf5time}).
(Column and row access timings were obtained with compressed column- and row-wise chunk layouts, respectively, which are discussed in more detail below.)
This suggests that the \code{HDF5Matrix} representation should be used sparingly -- if possible, smaller data sets should use alternative in-memory representations for faster access.

\begin{figure}[bt]
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/HDF5_col_ncol.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/HDF5_row_nrow.pdf}
        \caption{}
    \end{subfigure}
    \caption{Column and row access times for a HDF5-backed matrix.
        (a) Column access time with respect to increasing number of columns, for a dense matrix with 10000 rows.
        (b) Row access time with respect to increasing number of rows, for a dense matrix with 1000 columns.
        Horizontal dotted lines represent 2-fold increases in time.
    }
    \label{fig:hdf5time}
\end{figure}

A key determinant of the performance of the \code{HDF5Matrix} representation is the layout of data in the HDF5 file.
There are two layout choices for large matrices: contiguous or chunked.
In the contiguous layout, raw data are flattened into a one-dimensional array analogous to column-major storage of simple matrices in memory.
In the chunked layout, data are arranged into ``chunks'' of a pre-defined size.
For example, in a row-chunked layout, each chunk would only contain consecutive values from the same row of a matrix, i.e., chunks are nested within each row.
Similarly, with column-wise chunking, each chunk would be nested within a column.
Each chunk is always read (or written) in its entirety, even when only a portion of the chunk is requested.
Chunking is required for compression on disk, and more importantly, for fast access to data -- \textit{provided that the layout is consistent with the expected access pattern}.

We demonstrate the effect of the choice of file layout on access to HDF5-backed data.
For column access, we consider the performance of \beachmat{} with a contiguous layout, column-wise chunks with compression, and row-wise chunks.
The contiguous layout is the fastest as only one disk read needs to be performed per column (Figure~\ref{fig:hdf5layout}).
Column-wise chunking also involves a single disk read per column, but is slower due to the overhead of decompression and chunk caching.
Row-wise chunking is the slowest because access to a single column involves a disk read for every row chunk in its entirety, i.e., $N_r$ reads in total.
In fact, it is faster to convert (i.e., ``rechunk'') the entire data set to a column-wise chunk layout prior to performing column accesses via \beachmat{}.

\begin{figure}[bt]
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/HDF5_col_layout.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/simulations/pics/HDF5_row_layout.pdf}
        \caption{}
    \end{subfigure}
    \caption{Column/row access times for HDF5-backed matrices with different HDF5 file layouts, i.e., contiguous, row- or column-chunking with compression, and rechunking from one chunk layout to the optimal layout for each access pattern. 
        (a) Column access time with respect to increasing number of columns, for a dense matrix with 1000 rows.
        (b) Row access time with respect to increasing number of rows, for a dense matrix with 100 columns.
        Horizontal dotted lines represent 2-fold increases in time.
    }
    \label{fig:hdf5layout}
\end{figure}

For row access, row-wise chunking is the layout that provides the best performance with \beachmat{} (Figure~\ref{fig:hdf5layout}).
This is expected as only one disk read is required per row, compared to multiple reads with column-wise chunking.
(Again, rechunking to a row-wise layout is faster than perservering with a suboptimal column-wise layout for row access.)
The performance of the contiguous layout is poor as multiple reads are required to access the non-contiguous data from each row.
These results highlight the need to carefully choose a file layout that reflects the intended access pattern for optimal performance in C++ (and R) code.
Analysis pipelines should also be arranged such that steps requiring column access are separated from those requiring row access.
This minimizes the number of rechunking steps that need to be performed between steps.

As previously mentioned, chunking of the HDF5 file allows for compression of data using filters such as ZLIB and SZIP.
This decreases the size of the HDF5 file by at least 4-fold for dense matrices (80 MB to 19 MB for a 10000-by-1000 double-precision matrix), with even greater gains for sparse data (9 MB for the same matrix with 1\% non-zero entries).
The use of smaller files reduces the risk that disk space will be exceeded during the course of an analysis.
This is important when many on-disk matrices need to be constructed, e.g., to store transformed expression values or intermediate results.

\subsection*{Other matrix types}
While the matrix representations described above are the most commonly used for storing scRNA-seq data, \beachmat{} can be easily extended to other representations.
For example, the packed symmetric representation from the \textit{Matrix} package only stores the upper or lower half of a symmetric matrix.
This provides an efficient representation of distance matrices, which are often used to cluster cells based on their expression profiles.
\beachmat{} supports row and column access of data from packed symmetric matrices through the same interface that is used for the other representations.

\beachmat{} also supports data access from \code{RleMatrix} instances from the \textit{DelayedArray} package.
The \code{RleMatrix} stores its values as a column-major run-length encoding, where stretches of the same value in the one-dimensional array are stored as a single run.
This reduces memory usage in a more general manner than a sparse matrix, especially for matrices with many small but non-zero counts.
As with CSC matrices, \beachmat{} caches the row indices to speed up consecutive row access.

Another option for storing matrices on disk is to use the \textit{bigmemory} package \cite{kane2013scalable}.
This constructs an in-memory \code{big.matrix} object that contains external pointers pointing to an on-disk representation.
In \beachmat{}, we have deliberately chosen \code{HDF5Matrix} rather than \code{big.matrix} due to the standardized nature of the HDF5 specification and portability of HDF5 files across systems.
Nonetheless, we note that it is simple to extend \beachmat{} to accept \code{big.matrix} inputs if required.

\subsection*{Storing matrix output}
In addition to accessing data in existing matrices, the \beachmat{} API also supports the construction of various R matrices in C++.
For integer, logical and double-precision data, simple and HDF5-backed matrices can be constructed that are indistinguishable from those constructed in R.
Logical and double-precision data can also be stored in CSC format, where only true or non-zero values are retained in the \code{lgCMatrix} or \code{dgCMatrix} instance, respectively.
(The \textit{Matrix} package does not support sparse integer matrices.)
We do not support character matrix output yet, due to the complexity of handling variable-length strings in a manner that is compatible with both R and HDF5.

The output representation can either be explicitly specified in the code, or it can be automatically chosen to match some input representation.
To illustrate, consider a C++ function that accepts a matrix as input and returns a matrix of similar dimensions.
If the input is in the simple matrix format, one might assume that there is enough memory to also store the output in the simple format;
whereas if the input is a \code{HDF5Matrix}, one could presume that the output would be similarly large, thus requiring a \code{HDF5Matrix} representation for the results.
This facility means that results of processing in C++ can be readily returned in the most suitable representation for manipulation in R.
 
Note that the layout considerations described for HDF5-backed input are equally applicable to HDF5-backed output.
If the matrix is to be filled by row or column, the layout should be set to reflect this write access pattern for best performance.
This can be achieved by using functions from the \textit{HDF5Array} package to set the global chunking dimensions in R, which will be respected by the \beachmat{} API in the C++ code.

As an aside, the use of \beachmat{} may not be optimal for sparse inputs to functions that preserve sparsity (and thus return sparse outputs).
In such cases, a faster algorithm could be obtained by directly modifying the non-zero entries, rather than considering the zeroes during input and discarding them during output.
Of course, this would involve writing two different versions of code for sparse and non-sparse input/output, which is the scenario that \beachmat{} was designed to avoid.

\section*{Performance of \beachmat{} on real data sets}

\subsection*{Access times for the brain data set}
We evaluated the performance of \beachmat{} with the different matrix representations on real data, using the count matrix from a scRNA-seq study of the mouse brain \cite{zeisel2015brain}.
This data set contains integer expression values for 19972 genes in each of 3005 cells, of which 18\% are non-zero.
We note that this is not a particularly large matrix, especially in the context of droplet-based experiments that routinely generate data for tens of thousands of cells.
However, its size ensures that each of the matrix representations -- including those that are stored in memory -- can be easily evaluated and compared.

The performances of the different representations on the brain data set largely recapitulate the results with simulated data.
Row and column accesses from a simple matrix are the fastest, followed by accesses from a sparse matrix (Figure~\ref{fig:zeisel}).
HDF5-backed matrices provide the slowest access but also the smallest in-memory footprint (2 KB, compared to 480 MB for simple matrices and 130 MB for sparse matrices).
The on-disk size of the HDF5 file is also relatively small, requiring only 16-20 MB of space for each \code{HDF5Matrix} instance. 
These results demonstrate that the strengths and weaknesses of the different representations are present with real data.

\begin{figure}[bt]
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/real/pics/zeisel_col.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../timings/real/pics/zeisel_row.pdf}
        \caption{}
    \end{subfigure}
    \caption{Access times for various matrix representations of the mouse brain data set from Zeisel \textit{et al.} \cite{zeisel2015brain}.
        (a) Column access time for each representation, based on calculation of column sums.
        HDF5: column-chunked \code{HDF5Matrix},
        rechunk: row-chunked \code{HDF5Matrix} converted to a column-chunked layout.
        (b) Row access time for each representation, based on calculation of row sums.
        HDF5: row-chunked \code{HDF5Matrix},
        rechunk: column-chunked \code{HDF5Matrix} converted to a row-chunked layout.
        Heights represent the average of 10 repeats; standard errors were negligible and not shown.
    }
    \label{fig:zeisel}
\end{figure}

\subsection*{Analysis of a large data set}

\section*{Discussion}
% Some words about disappearing sparsity.

\section*{Methods}

\subsection*{Access timings with simulated data}
Double-precision dense matrices of a specified dimension were filled with values sampled from a standard normal distribution.
By default, all dense matrices were constructed in the simple format.
Double-precision CSC matrices of a specified dimension and density were generated with the \code{rsparsematrix} function from the \textit{Matrix} package. 
These were constructed as \code{dgCMatrix} instances.
Conversion of each matrix object into other representations was performed prior to timing access with the C++ APIs.

To time row and column access with \beachmat{} and other APIs, we wrote a C++ function that computes the sum of values in each row or column, respectively.
Calculation of the row/column sums ensures that each entry of the row/column is visited in order to use its value.
This means that each API has to do some work, avoiding trivially fast approaches where a pointer or iterator is returned to the start of the column/row.
Summation is also simple enough that the access time of the API will still constitute a major part of the overall time spent by the function.

Timings were performed in R using the \code{system.time} function on a call to each C++ function via \code{.Call}. 
This was repeated 10 times with new matrices, and the average time and standard error was computed for each access method. 
Row and column access times were evaluated with respect to increasing numbers of rows and columns, respectively.
For sparse matrices and to test the effects of HDF5 compression, results were also recorded with respect to increasing density of non-zero entries.

\subsection*{Rechunking of the HDF5 file layout}
We developed a simple yet efficient approach for converting between row-wise and column-wise chunking in a HDF5 file.
Consider an existing file with chunk dimensions of $(p_1, 1)$ for $p_1$ rows and 1 column, i.e., column-wise chunking where each chunk contains $p_1$ values.
Assume that we want to obtain a new file with chunk dimensions of $(1, p_2)$, i.e., row-wise chunks containing $p_2$ values.
We load $p_2$ column-wise chunks from the original file into memory at once, and write them into a new file as $p_1$ row-wise chunks.
This is repeated until all data are copied from the original to the new file.
The reverse procedure can be performed to convert from row-wise chunks to column-wise chunks.

Our rechunking approach minimizes the number of disk accesses by ensuring that each read/write processes all data from a single chunk.
However, it assumes that the space required by $p_1p_2$ values is small enough to fit into memory. 
This is usually not an issue, e.g., for $p_1=p_2=10^4$ with double-precision data, each read/write iteration would only require 800 MB of memory.
In cases where $p_1$ is very large, it is possible to first convert to row-wise chunks of size $p'_2$ (where $p'_2$ is some factor of $p_2$) in an intermediate file.
The intermediate file can then be easily processed into a new file containing row-wise chunks of size $p_2$, requiring only $p_2/p'_2$ chunks to be held in memory.

For all procedures involving chunked HDF5 files, we used column- or row-wise chunks containing 5000 double-precision values.
If any chunk dimension exceeded the corresponding dimension of the matrix, the chunk size was set to the exceeded dimension instead.
We restricted ourselves to column- and row-wise chunks as these are optimal for access to any single column or row, respectively.
Obviously, other layouts are possible but their performances will depend on other factors (e.g., the size of the chunk cache, the order of access of rows/columns) and will not be considered here.

\subsection*{Access timings with the brain data}
scRNA-seq data from the mouse brain study \cite{zeisel2015brain} were obtained as a count matrix from \url{http://linnarssonlab.org/cortex/}.
Counts were read into R and converted into a double-precision simple matrix, a \code{dgCMatrix} or a \code{HDF5Matrix}.
For each representation, timings of the calculation of row or column sums were performed as previously described.
This was repeated 10 times to obtain an average time.
Column- and row-wise chunking were used for timing column and row access, respectively, of a \code{HDF5Matrix}.

{\small
    \bibliography{ref}
    \bibliographystyle{abbrv}
}


\end{document}
